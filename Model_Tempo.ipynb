{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What does the data look like?"
      ],
      "metadata": {
        "id": "1YeHfMCJnZrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_data_dp = pd.read_csv(\"/content/dp_rep1.csv\")\n",
        "df_data_norm = pd.read_csv(\"/content/norm_rep1.csv\")\n",
        "df_data_sp = pd.read_csv(\"/content/sp_rep1.csv\")\n",
        "\n",
        "df_data_dp.tail(), df_data_norm.head(), df_data_sp.head()"
      ],
      "metadata": {
        "id": "qAExfd_p1XU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Data"
      ],
      "metadata": {
        "id": "NZzG6JEK4EG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def transform_dataframe(df):\n",
        "    # Extract the first column and transpose\n",
        "    first_col = df.iloc[:, 0].values.reshape(1, -1)\n",
        "\n",
        "    # Extract the fifth column's value\n",
        "    fifth_col_value = df.iloc[0, 4]  # Assuming you want the first row of the fifth column\n",
        "\n",
        "    # Combine\n",
        "    combined = np.hstack((first_col, [[fifth_col_value]]))\n",
        "\n",
        "    return combined\n",
        "\n",
        "# List of file paths\n",
        "train_filepaths = [\n",
        "    '/content/dp_rep1.csv', '/content/dp_rep2.csv', '/content/dp_rep3.csv', '/content/dp_rep4.csv', '/content/dp_rep5.csv', '/content/dp_rep6.csv', '/content/dp_rep7.csv', '/content/dp_rep8.csv',\n",
        "    '/content/norm_rep1.csv', '/content/norm_rep2.csv', '/content/norm_rep3.csv', '/content/norm_rep4.csv', '/content/norm_rep5.csv', '/content/norm_rep6.csv', '/content/norm_rep7.csv', '/content/norm_rep8.csv',\n",
        "    '/content/sp_rep1.csv', '/content/sp_rep2.csv', '/content/sp_rep3.csv', '/content/sp_rep4.csv', '/content/sp_rep5.csv', '/content/sp_rep6.csv', '/content/sp_rep7.csv', '/content/sp_rep8.csv'\n",
        "]\n",
        "\n",
        "# Iterate over file paths, read each CSV, transform, and concatenate\n",
        "transformed_dfs = [transform_dataframe(pd.read_csv(filepath)) for filepath in train_filepaths]\n",
        "result = np.vstack(transformed_dfs)\n",
        "\n",
        "result.shape"
      ],
      "metadata": {
        "id": "q1e31fJJ1ZxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encode Tempo"
      ],
      "metadata": {
        "id": "3IUOYgpUm3Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Split the data and labels\n",
        "data = result[:, :-1]\n",
        "labels = result[:, -1].reshape(-1, 1)  # reshape to make it 2D\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "labels_one_hot = encoder.fit_transform(labels)\n",
        "\n",
        "# Concatenate the data and one-hot encoded labels\n",
        "result = np.hstack((data, labels_one_hot))\n",
        "result = result.astype(np.float64)\n",
        "\n",
        "np.random.shuffle(result)\n",
        "\n",
        "x_train_tempo = result[:, :-3]\n",
        "y_train_tempo = result[:, -3:]\n",
        "\n",
        "x_train_tempo.shape, y_train_tempo.shape"
      ],
      "metadata": {
        "id": "VEqqB6Bs1ovT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation Set\n"
      ],
      "metadata": {
        "id": "sK04rvrQ2Ar2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def transform_dataframe(df):\n",
        "    # Extract the first column and transpose\n",
        "    first_col = df.iloc[:, 0].values.reshape(1, -1)\n",
        "\n",
        "    # Extract the fifth column's value\n",
        "    fifth_col_value = df.iloc[0, 4]  # Assuming you want the first row of the fifth column\n",
        "\n",
        "    # Combine\n",
        "    combined = np.hstack((first_col, [[fifth_col_value]]))\n",
        "\n",
        "    return combined\n",
        "\n",
        "# List of file paths\n",
        "validation_filepaths = [\n",
        "    '/content/dp_rep9.csv', '/content/dp_rep10.csv',\n",
        "    '/content/norm_rep9.csv', '/content/norm_rep10.csv',\n",
        "    '/content/sp_rep9.csv', '/content/sp_rep10.csv'\n",
        "]\n",
        "\n",
        "# Iterate over file paths, read each CSV, transform, and concatenate\n",
        "transformed_dfs = [transform_dataframe(pd.read_csv(filepath)) for filepath in validation_filepaths]\n",
        "result = np.vstack(transformed_dfs)\n",
        "\n",
        "result.shape"
      ],
      "metadata": {
        "id": "b2saftbp2Nr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encode Tempo"
      ],
      "metadata": {
        "id": "CXLKcJKd2Dwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Split the data and labels\n",
        "data = result[:, :-1]\n",
        "labels = result[:, -1].reshape(-1, 1)  # reshape to make it 2D\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "labels_one_hot = encoder.fit_transform(labels)\n",
        "\n",
        "# Concatenate the data and one-hot encoded labels\n",
        "result = np.hstack((data, labels_one_hot))\n",
        "result = result.astype(np.float64)\n",
        "np.random.shuffle(result)\n",
        "\n",
        "x_validation_tempo = result[:, :-3]\n",
        "y_validation_tempo = result[:, -3:]\n",
        "\n",
        "x_validation_tempo.shape, y_validation_tempo.shape"
      ],
      "metadata": {
        "id": "jICQivf52dy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Data"
      ],
      "metadata": {
        "id": "hgXz2sBVmyjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def transform_dataframe(df):\n",
        "    # Extract the first column and transpose\n",
        "    first_col = df.iloc[:, 0].values.reshape(1, -1)\n",
        "\n",
        "    # Extract the fifth column's value\n",
        "    fifth_col_value = df.iloc[0, 4]  # Assuming you want the first row of the fifth column\n",
        "\n",
        "    # Combine\n",
        "    combined = np.hstack((first_col, [[fifth_col_value]]))\n",
        "\n",
        "    return combined\n",
        "\n",
        "# List of file paths\n",
        "test_filepaths = [\n",
        "    '/content/dp_rep11.csv', '/content/dp_rep12.csv',\n",
        "    '/content/norm_rep11.csv', '/content/norm_rep12.csv',\n",
        "    '/content/sp_rep11.csv', '/content/sp_rep12.csv'\n",
        "]\n",
        "\n",
        "# Iterate over file paths, read each CSV, transform, and concatenate\n",
        "transformed_dfs = [transform_dataframe(pd.read_csv(filepath)) for filepath in test_filepaths]\n",
        "result = np.vstack(transformed_dfs)\n",
        "\n",
        "result.shape"
      ],
      "metadata": {
        "id": "KqCMKUst1vbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encode Tempo"
      ],
      "metadata": {
        "id": "OC3FmdTwqnk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Split the data and labels\n",
        "data = result[:, :-1]\n",
        "labels = result[:, -1].reshape(-1, 1)  # reshape to make it 2D\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "labels_one_hot = encoder.fit_transform(labels)\n",
        "\n",
        "# Concatenate the data and one-hot encoded labels\n",
        "result = np.hstack((data, labels_one_hot))\n",
        "result = result.astype(np.float64)\n",
        "np.random.shuffle(result)\n",
        "\n",
        "x_test_tempo = result[:, :-3]\n",
        "y_test_tempo = result[:, -3:]\n",
        "\n",
        "x_test_tempo.shape, y_test_tempo.shape"
      ],
      "metadata": {
        "id": "rPzbY1n01xG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardize the Training, Validation and Testing Data"
      ],
      "metadata": {
        "id": "hJ5irS5zrMhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def row_standardize(data):\n",
        "    row_means = np.mean(data, axis=1, keepdims=True)\n",
        "    row_stds = np.std(data, axis=1, keepdims=True)\n",
        "    return (data - row_means) / (row_stds + 1e-10)  # adding a small value to avoid division by zero\n",
        "\n",
        "x_train_tempo_std = row_standardize(x_train_tempo)\n",
        "x_validation_tempo_std = row_standardize(x_validation_tempo)\n",
        "x_test_tempo_std = row_standardize(x_test_tempo)\n",
        "\n",
        "x_train_tempo_std.shape, x_validation_tempo_std.shape, x_test_tempo_std.shape"
      ],
      "metadata": {
        "id": "R2lL_tIp4RqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model v4.2\n"
      ],
      "metadata": {
        "id": "H3DCybJGdeAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(6000,)),  # Input layer for 6000 features\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5)\n",
        "    layers.Dense(3, activation='softmax')  # Output layer for 3 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=[tfa.metrics.F1Score(num_classes=3, average='macro')])\n",
        "\n",
        "# model summary\n",
        "model.summary()\n",
        "\n",
        "print(x_train_tempo_std.shape, y_train_tempo.shape, x_test_tempo_std.shape, y_test_tempo.shape)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train_tempo_std, y_train_tempo,\n",
        "    epochs=50,\n",
        "    batch_size=8,\n",
        "    validation_data=(x_validation_tempo_std, y_validation_tempo),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "DTSaKVozCOP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "Kq2xmUXYtl8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the training set\n",
        "train_loss, train_accuracy = model.evaluate(x_train_tempo_std, y_train_tempo)\n",
        "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "test_loss, test_accuracy = model.evaluate(x_test_tempo_std, y_test_tempo)\n",
        "print(f\"Validation accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "f65l8B1p29qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrics"
      ],
      "metadata": {
        "id": "cYRHZTwaCdFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict the classes for the testing set\n",
        "y_pred = model.predict(x_test_tempo_std)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded ground truth labels back to class labels\n",
        "y_true = np.argmax(y_test_tempo, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt='g', cmap='Blues', xticklabels=['Deadpan', 'Normal', 'Speed'], yticklabels=['Deadpan', 'Normal', 'Speed'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SuGc9A0_Cakn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardise Prediction Data"
      ],
      "metadata": {
        "id": "5i0dQW1M4cJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_filepaths = [\n",
        "    '/content/dp_rep12.csv'\n",
        "]\n",
        "\n",
        "# Iterate over file paths, read each CSV, transform, and concatenate\n",
        "transformed_dfs = [transform_dataframe(pd.read_csv(filepath)) for filepath in pred_filepaths]\n",
        "result_p = np.vstack(transformed_dfs)\n",
        "\n",
        "result_p.shape\n",
        "\n",
        "# Split the data and labels\n",
        "data = result_p[:, :-1]\n",
        "labels = result_p[:, -1].reshape(-1, 1)  # reshape to make it 2D\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "labels_one_hot = encoder.fit_transform(labels)\n",
        "\n",
        "# Concatenate the data and one-hot encoded labels\n",
        "result = np.hstack((data, labels_one_hot))\n",
        "result = result.astype(np.float64)\n",
        "np.random.shuffle(result)\n",
        "\n",
        "x_pred_tempo = result_p[:, :-3]\n",
        "y_pred_tempo = result_p[:, -3:]\n",
        "\n",
        "x_pred_tempo.shape, y_pred_tempo.shape\n",
        "\n",
        "def row_standardize(data):\n",
        "    row_means = np.mean(data, axis=1, keepdims=True)\n",
        "    row_stds = np.std(data, axis=1, keepdims=True)\n",
        "    return (data - row_means) / (row_stds + 1e-10)  # adding a small value to avoid division by zero\n",
        "\n",
        "x_pred_tempo = row_standardize(x_pred_tempo)\n",
        "\n",
        "x_pred_tempo.shape"
      ],
      "metadata": {
        "id": "yXSz5Enn38-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "IUTzf4AG7mqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_pred_tempo)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "W32DnS2j3wMm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}